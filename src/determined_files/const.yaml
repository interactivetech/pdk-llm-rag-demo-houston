name: mistral_finetune_hpe
workspace: Andrew
project: Mistral
description: "Mistral"

hyperparameters:
    global_batch_size: 1
    weight_decay: 0.001
    learning_rate: 0.00005
    adam_epsilon: 1e-8
    warmup_steps: 3
    gradient_accumulation_steps: 1
    # dataset_name: 'PDS2'
    model_cache_dir: '/mnt/efs/shared_fs/determined/hf_cache/mistral_instruct_model_cache/'
    tokenizer_cache_dir: '/mnt/efs/shared_fs/determined/hf_cache/mistral_instruct_tokenizer_cache/'
    dataset_cache_dir: '/mnt/efs/shared_fs/determined/hf_cache/dataset_tokenizer_cache/'
    dataset_json_file: '/mnt/efs/shared_fs/determined/hf_cache/dataset_download_json_dir.json'
    finetune_results_dir: '/mnt/efs/shared_fs/determined/hf_cache/mistral_ft_results/'
    dataset_name: 'hpe_press_release'
bind_mounts:
  - container_path: /mnt/
    host_path: /mnt
    read_only: false
environment:
    image: "mendeza/mistral-rag-env:0.0.1"
records_per_epoch: 64 
resources:
    slots_per_trial: 1
min_validation_period:
  batches: 2
checkpoint_policy: none
checkpoint_storage:
  host_path: /mnt/efs/shared_fs/determined
  propagation: rprivate
  save_experiment_best: 0
  save_trial_best: 0
  save_trial_latest: 0
  storage_path: null
  type: shared_fs
profiling:
  begin_on_batch: 0
  enabled: true
min_checkpoint_period:
  batches: 2
searcher:
    name: single
    metric: eval_loss
    max_length:
        batches: 2
    smaller_is_better: true
max_restarts: 0
entrypoint: python3 -m determined.launch.torch_distributed --trial model_def:MistralFinetuneTrial